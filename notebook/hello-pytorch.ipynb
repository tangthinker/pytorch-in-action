{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f2e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello pytorch!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello pytorch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da6897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0deb3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d016f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8718, 0.4817, 0.0399],\n",
       "        [0.4085, 0.1511, 0.2417],\n",
       "        [0.7388, 0.7982, 0.9518],\n",
       "        [0.3453, 0.9646, 0.5266],\n",
       "        [0.7929, 0.1881, 0.2295]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02baddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67f39f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bb1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0acb3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eedb4763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.zeros(2, 5, 3)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3733ddff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba633c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f18e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2417)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc83e270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0399, 0.2417, 0.9518, 0.5266, 0.2295])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec3869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6a9084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8718, 1.4817, 1.0399],\n",
       "        [1.4085, 1.1511, 1.2417],\n",
       "        [1.7388, 1.7982, 1.9518],\n",
       "        [1.3453, 1.9646, 1.5266],\n",
       "        [1.7929, 1.1881, 1.2295]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e73e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3934, 1.3934, 1.3934, 1.3934, 1.3934],\n",
       "        [0.8012, 0.8012, 0.8012, 0.8012, 0.8012],\n",
       "        [2.4888, 2.4888, 2.4888, 2.4888, 2.4888],\n",
       "        [1.8365, 1.8365, 1.8365, 1.8365, 1.8365],\n",
       "        [1.2106, 1.2106, 1.2106, 1.2106, 1.2106]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = x.mm(y.t())\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa4408ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3726,  0.3835, -0.1237],\n",
       "        [-0.7973,  0.4626, -0.0668]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_tensor = torch.randn(2,3)\n",
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b760c945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32581136,  0.92988396, -0.074781  ],\n",
       "       [ 1.91309714, -0.84458311,  0.25418791]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_numpy = np.random.randn(2,3)\n",
    "y_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44c73235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3725793 ,  0.38354534, -0.12371133],\n",
       "       [-0.79726905,  0.46256185, -0.06680126]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_numpy = x_tensor.numpy()\n",
    "x_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "740cb7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3258,  0.9299, -0.0748],\n",
       "        [ 1.9131, -0.8446,  0.2542]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor = torch.from_numpy(y_numpy)\n",
    "y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e31c98be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e68b62d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "390b888a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7b5f041bfd90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b02891cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/486760323.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28a66443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7b5f041bfd90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9813387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 9.],\n",
       "        [9., 9.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09d44834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x7b5f041bfb50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb16893a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.mean(z)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57104839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MeanBackward0 at 0x7b5f041bfe50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78441ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d76b2efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/3566409831.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(t.grad)\n"
     ]
    }
   ],
   "source": [
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b25dfc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/2878411509.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(z.grad)\n"
     ]
    }
   ],
   "source": [
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50d2152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/1324337507.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91ba4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5000, 1.5000],\n",
      "        [1.5000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e294a8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100, 0.0200]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.tensor([[0.01, 0.02]], requires_grad=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21361fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e5217f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = s.mm(x)\n",
    "\n",
    "z = torch.mean(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb037570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.3600, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84e93eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37.1200, 37.1200],\n",
      "        [39.6800, 39.6800]])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/3606434708.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(s.grad)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(s.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ff9c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cda1f2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c111a947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [4., 4.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95fabb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f449b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/4110045842.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  z.grad\n"
     ]
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b743e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [4., 4.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72cd71a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7b5f041bc640>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e8386db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0cede86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x7b5f041fd600>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15737986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x7b5f041fcac0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum().grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5688412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(10., requires_grad=True)\n",
    "y = x + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93a3f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f1834cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90b41cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141386/486760323.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647175880/work/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5779cb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[10., 10.]], requires_grad=True)\n",
    "y = x + 1\n",
    "z = torch.mean(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c323432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d55ec56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d0248b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "767e547e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c51f5761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00d637ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(1, -1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f0ee05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(-1, 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a40b3bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1000.0, 2000.0, 30000.0])\n",
    "softmax = torch.functional.F.softmax(x, dim=0)\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e1e7d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = torch.exp(x) / torch.exp(x).sum()\n",
    "log_softmax = torch.log(softmax)\n",
    "log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aaa1485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-29000., -28000.,      0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax = torch.functional.F.log_softmax(x, dim=0)\n",
    "log_softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
